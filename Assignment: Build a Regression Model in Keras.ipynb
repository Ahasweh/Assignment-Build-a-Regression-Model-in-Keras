{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Build a Regression Model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requored Libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==2.0.2 in ./dl_env/lib/python3.12/site-packages (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas==2.2.2 in ./dl_env/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./dl_env/lib/python3.12/site-packages (from pandas==2.2.2) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./dl_env/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./dl_env/lib/python3.12/site-packages (from pandas==2.2.2) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./dl_env/lib/python3.12/site-packages (from pandas==2.2.2) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./dl_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tensorflow==2.18.0 in ./dl_env/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./dl_env/lib/python3.12/site-packages (from tensorflow==2.18.0) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./dl_env/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
      "Requirement already satisfied: rich in ./dl_env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.9.4)\n",
      "Requirement already satisfied: namex in ./dl_env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\n",
      "Requirement already satisfied: optree in ./dl_env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./dl_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./dl_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./dl_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./dl_env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./dl_env/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./dl_env/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./dl_env/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./dl_env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./dl_env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./dl_env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./dl_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in ./dl_env/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./dl_env/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./dl_env/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./dl_env/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./dl_env/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install all Libraries required for this assignment are listed below. \n",
    "\n",
    "!pip install numpy==2.0.2\n",
    "!pip install pandas==2.2.2\n",
    "!pip install tensorflow==2.18.0\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Build a baseline model (5 marks) \n",
    "\n",
    "Use the Keras library to build a neural network with the following:\n",
    "\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "\n",
    "- Use the adam optimizer and the mean squared error  as the loss function.\n",
    "\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the \n",
    "train_test_split\n",
    "helper function from Scikit-learn.\n",
    "\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "\n",
    "5. Report the mean and the standard deviation of the mean squared errors.\n",
    "\n",
    "Submit your Jupyter Notebook with your code and comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the Keras, Pandas, and Numpy Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Download the dataset into pandas library \n",
    "# Please download the data to your project folder before calling this code \n",
    "\n",
    "filepath='concrete_data.csv'\n",
    "concrete_data = pd.read_csv(filepath)\n",
    "\n",
    "concrete_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',\n",
      "       'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read data column names\n",
    "concrete_data_columns = concrete_data.columns\n",
    "print(concrete_data_columns)\n",
    "\n",
    "# Split data into predictors and target data\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regression model\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(8,)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 - 0s - 15ms/step - loss: 894068.6250 - val_loss: 719990.5625\n",
      "Epoch 2/50\n",
      "23/23 - 0s - 2ms/step - loss: 577362.3125 - val_loss: 461975.0312\n",
      "Epoch 3/50\n",
      "23/23 - 0s - 2ms/step - loss: 367028.2500 - val_loss: 294562.2500\n",
      "Epoch 4/50\n",
      "23/23 - 0s - 2ms/step - loss: 232416.1406 - val_loss: 187588.6094\n",
      "Epoch 5/50\n",
      "23/23 - 0s - 2ms/step - loss: 146392.0312 - val_loss: 118227.7812\n",
      "Epoch 6/50\n",
      "23/23 - 0s - 2ms/step - loss: 90868.9297 - val_loss: 73260.1328\n",
      "Epoch 7/50\n",
      "23/23 - 0s - 2ms/step - loss: 55451.5664 - val_loss: 44732.9141\n",
      "Epoch 8/50\n",
      "23/23 - 0s - 2ms/step - loss: 31985.2949 - val_loss: 22767.0449\n",
      "Epoch 9/50\n",
      "23/23 - 0s - 2ms/step - loss: 13772.6230 - val_loss: 8885.8408\n",
      "Epoch 10/50\n",
      "23/23 - 0s - 2ms/step - loss: 8144.8696 - val_loss: 7748.4829\n",
      "Epoch 11/50\n",
      "23/23 - 0s - 2ms/step - loss: 7555.2148 - val_loss: 7415.3696\n",
      "Epoch 12/50\n",
      "23/23 - 0s - 2ms/step - loss: 7198.9087 - val_loss: 6982.9824\n",
      "Epoch 13/50\n",
      "23/23 - 0s - 2ms/step - loss: 6871.2607 - val_loss: 6661.1162\n",
      "Epoch 14/50\n",
      "23/23 - 0s - 2ms/step - loss: 6608.6250 - val_loss: 6367.6196\n",
      "Epoch 15/50\n",
      "23/23 - 0s - 2ms/step - loss: 6356.9453 - val_loss: 6149.9741\n",
      "Epoch 16/50\n",
      "23/23 - 0s - 2ms/step - loss: 6144.0063 - val_loss: 5906.6113\n",
      "Epoch 17/50\n",
      "23/23 - 0s - 2ms/step - loss: 5933.3569 - val_loss: 5667.3271\n",
      "Epoch 18/50\n",
      "23/23 - 0s - 2ms/step - loss: 5736.5254 - val_loss: 5490.5967\n",
      "Epoch 19/50\n",
      "23/23 - 0s - 2ms/step - loss: 5551.3418 - val_loss: 5298.1875\n",
      "Epoch 20/50\n",
      "23/23 - 0s - 2ms/step - loss: 5382.8389 - val_loss: 5120.8735\n",
      "Epoch 21/50\n",
      "23/23 - 0s - 2ms/step - loss: 5226.2622 - val_loss: 4958.5034\n",
      "Epoch 22/50\n",
      "23/23 - 0s - 2ms/step - loss: 5067.3906 - val_loss: 4815.3076\n",
      "Epoch 23/50\n",
      "23/23 - 0s - 2ms/step - loss: 4927.9692 - val_loss: 4652.9893\n",
      "Epoch 24/50\n",
      "23/23 - 0s - 2ms/step - loss: 4795.2686 - val_loss: 4532.6270\n",
      "Epoch 25/50\n",
      "23/23 - 0s - 2ms/step - loss: 4669.9058 - val_loss: 4410.4048\n",
      "Epoch 26/50\n",
      "23/23 - 0s - 2ms/step - loss: 4543.0811 - val_loss: 4280.8315\n",
      "Epoch 27/50\n",
      "23/23 - 0s - 2ms/step - loss: 4427.9326 - val_loss: 4160.5986\n",
      "Epoch 28/50\n",
      "23/23 - 0s - 2ms/step - loss: 4327.1777 - val_loss: 4050.7427\n",
      "Epoch 29/50\n",
      "23/23 - 0s - 2ms/step - loss: 4209.0166 - val_loss: 3926.1323\n",
      "Epoch 30/50\n",
      "23/23 - 0s - 2ms/step - loss: 4109.2031 - val_loss: 3840.7954\n",
      "Epoch 31/50\n",
      "23/23 - 0s - 2ms/step - loss: 4005.1367 - val_loss: 3726.0405\n",
      "Epoch 32/50\n",
      "23/23 - 0s - 2ms/step - loss: 3912.6013 - val_loss: 3649.3613\n",
      "Epoch 33/50\n",
      "23/23 - 0s - 2ms/step - loss: 3819.0093 - val_loss: 3543.3984\n",
      "Epoch 34/50\n",
      "23/23 - 0s - 2ms/step - loss: 3733.3135 - val_loss: 3463.8345\n",
      "Epoch 35/50\n",
      "23/23 - 0s - 2ms/step - loss: 3649.7212 - val_loss: 3373.4946\n",
      "Epoch 36/50\n",
      "23/23 - 0s - 2ms/step - loss: 3563.5249 - val_loss: 3281.3245\n",
      "Epoch 37/50\n",
      "23/23 - 0s - 2ms/step - loss: 3478.1553 - val_loss: 3220.7693\n",
      "Epoch 38/50\n",
      "23/23 - 0s - 2ms/step - loss: 3399.3110 - val_loss: 3134.5698\n",
      "Epoch 39/50\n",
      "23/23 - 0s - 2ms/step - loss: 3321.8335 - val_loss: 3063.0229\n",
      "Epoch 40/50\n",
      "23/23 - 0s - 2ms/step - loss: 3250.3127 - val_loss: 2987.6575\n",
      "Epoch 41/50\n",
      "23/23 - 0s - 2ms/step - loss: 3174.6924 - val_loss: 2930.5657\n",
      "Epoch 42/50\n",
      "23/23 - 0s - 2ms/step - loss: 3106.0923 - val_loss: 2845.0815\n",
      "Epoch 43/50\n",
      "23/23 - 0s - 2ms/step - loss: 3033.1931 - val_loss: 2800.7080\n",
      "Epoch 44/50\n",
      "23/23 - 0s - 3ms/step - loss: 2965.2971 - val_loss: 2721.2651\n",
      "Epoch 45/50\n",
      "23/23 - 0s - 2ms/step - loss: 2900.2708 - val_loss: 2658.3696\n",
      "Epoch 46/50\n",
      "23/23 - 0s - 2ms/step - loss: 2835.9417 - val_loss: 2602.0864\n",
      "Epoch 47/50\n",
      "23/23 - 0s - 2ms/step - loss: 2777.6443 - val_loss: 2545.3320\n",
      "Epoch 48/50\n",
      "23/23 - 0s - 2ms/step - loss: 2712.1418 - val_loss: 2477.6038\n",
      "Epoch 49/50\n",
      "23/23 - 0s - 2ms/step - loss: 2654.5745 - val_loss: 2431.8794\n",
      "Epoch 50/50\n",
      "23/23 - 0s - 2ms/step - loss: 2597.6348 - val_loss: 2383.4463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x12ecc2a80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = regression_model()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Train the model on the training data using 50 epochs\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "[84.34515831099493, 153.72004512404698, 2366.3329426995833, 649.3613388976289, 125.27095931614562, 284.7488897459891, 138.69100317284884, 342.305808881914, 115.55407206147238, 164.20117271897942, 173.1645739310996, 180.25495150601202, 127.59363609321309, 274.87747027844506, 2547.2983351184566, 114.49296692300493, 112.14281067442492, 487.47942636589926, 101.30281024555741, 263.9506457542997, 195.77576335939085, 202.96703191048036, 189.1978823346955, 2544.666056647427, 108.19861892680866, 171.4190551159274, 172.30293361019764, 529.8546035913461, 633.360734932112, 174.32130770709313, 153.9409754418889, 201.05034483463928, 81.6542485222245, 111.95379676380435, 117.25515105755237, 284.9337098498942, 207.945763376215, 328.27372290006383, 254.14725433121401, 1420.8363795194023, 329.7844757417535, 156.51546530414035, 117.46744370007308, 106.62030138297565, 105.02050691589785, 693.5793929023479, 370.4888325391418, 118.02969412173108, 217.90371015658022, 147.1849304849103]\n"
     ]
    }
   ],
   "source": [
    "# List to store mean squared errors\n",
    "mse_list = []\n",
    "\n",
    "# Repeat the split and train steps 50 times\n",
    "for i in range(50):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = regression_model()\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Now this will work\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Print the list of mean squared errors\n",
    "print(mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Mean Squared Errors: 385.07478211603893\n",
      "Standard Deviation of Mean Squared Errors: 575.4091683716704\n"
     ]
    }
   ],
   "source": [
    "#  Report the mean and the standard deviation of the mean squared errors.\n",
    "\n",
    "mean_mse = np.mean(mse_list)  # Calculate the mean of the MSEs\n",
    "std_mse = np.std(mse_list)    # Calculate the standard deviation of the MSEs\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean of Mean Squared Errors: {mean_mse}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Normalize the data (5 marks) \n",
    "\n",
    "Repeat Part A but use a normalized version of the data. Recall that one way to normalize the data is by subtracting the mean from the individual predictors and dividing by the standard deviation.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "[395.80327503226505, 433.0468632592371, 341.76081758027635, 319.2908527191814, 400.63038797221117, 452.35946649956975, 387.07822091415227, 305.1334785821643, 320.8160253473356, 235.20662337476375, 264.89753579280347, 439.4727567202748, 442.0808895364478, 403.1995144674792, 378.69111569801254, 475.4596993318306, 283.4193707845494, 292.83622287036167, 377.53629105238434, 470.131187223128, 426.6156155874387, 362.483371246618, 385.43818618873934, 275.1059106694571, 306.59467177960994, 325.93876551236633, 466.67976293791605, 293.684075487761, 322.6760681958537, 387.2646884256472, 480.97846477995614, 255.22580173884305, 279.82459934212017, 838.675846601696, 399.93529410347725, 353.2550357649056, 238.59591982891598, 334.68365338925145, 480.3136177315525, 369.78207535810856, 388.3704710805031, 252.31371299578004, 289.39650334498765, 504.99373634036084, 276.9468903989549, 436.2606547041671, 265.61949683848957, 417.5845038965052, 302.5852685569269, 465.5989689104689]\n",
      "Mean of Mean Squared Errors: 372.04544452991615\n",
      "Standard Deviation of Mean Squared Errors: 99.76220916014996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# normalize the predictors data\n",
    "predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "predictors_norm.head()\n",
    "\n",
    "# List to store mean squared errors\n",
    "mse_list = []\n",
    "\n",
    "\n",
    "# Repeat the split and train steps 50 times\n",
    "for i in range(50):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = regression_model()\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Now this will work\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Print the list of mean squared errors\n",
    "print(mse_list)\n",
    "\n",
    "#  Report the mean and the standard deviation of the mean squared errors.\n",
    "\n",
    "mean_mse = np.mean(mse_list)  # Calculate the mean of the MSEs\n",
    "std_mse = np.std(mse_list)    # Calculate the standard deviation of the MSEs\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean of Mean Squared Errors: {mean_mse}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the mean of the mean squared errors compare to that from Step A?\n",
    "\n",
    "#### Answer: the mean of the mean squared errors has been reduced from 385.07 to 372.045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Increate the number of epochs (5 marks)\n",
    "\n",
    "Repeat Part B but use 100 epochs this time for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "[232.9763266589723, 175.73837573636786, 151.42513753503871, 175.42464492652854, 202.11630500751, 169.59308390937966, 185.52849653564638, 147.13084105706542, 164.52493817938534, 144.09406735522353, 155.96812994514494, 146.35865993928084, 161.39715512799424, 185.19148183196094, 167.51184084772643, 174.47533267079476, 166.93078325813326, 145.52496259209758, 135.98363280213033, 169.40777293388038, 160.23216944608163, 157.92747551354753, 152.2440335650143, 193.82596475738515, 156.07706559663362, 193.1223642708291, 180.55334368788132, 193.96175170291193, 164.47371728029287, 157.34181416678558, 183.38287261195583, 152.85407710592702, 151.77922821554273, 150.25437280906814, 172.51369070110027, 162.71339284580242, 153.07487742528423, 160.6515424801132, 173.3633380780256, 165.01032285141756, 163.7618431038488, 168.67082898856214, 171.83085369646972, 160.82797033170255, 175.55762067971386, 168.71399339856225, 196.35369578828053, 193.70596596152876, 143.54192440526862, 149.63567318555772]\n",
      "Mean of Mean Squared Errors: 167.70519507002712\n",
      "Standard Deviation of Mean Squared Errors: 18.1024573187035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Repeat the split and train steps 100 times\n",
    "mse_list = []\n",
    "\n",
    "for i in range(50):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = regression_model()\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Now this will work\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Print the list of mean squared errors\n",
    "print(mse_list)\n",
    "\n",
    "#  Report the mean and the standard deviation of the mean squared errors.\n",
    "\n",
    "mean_mse = np.mean(mse_list)  # Calculate the mean of the MSEs\n",
    "std_mse = np.std(mse_list)    # Calculate the standard deviation of the MSEs\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean of Mean Squared Errors: {mean_mse}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the mean of the mean squared errors compare to that from Step B?\n",
    "#### the mean has been signgactly reduced to 167.705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Increase the number of hidden layers (5 marks)\n",
    "\n",
    "Repeat part B but use a neural network with the following instead:\n",
    "\n",
    "- Three hidden layers, each of 10 nodes and ReLU activation function.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80.23801766930869, 129.9704687660414, 136.32393083113305, 127.79804742556703, 116.17345298002336, 138.3066689915849, 156.4241336811637, 120.67092327657848, 145.97599292635377, 127.77449989762476, 122.07913399867223, 124.19341688530976, 130.7506097950394, 114.62580537165393, 108.67745701877598, 106.45151665272991, 110.66608831523355, 107.93373095358545, 70.7099558851428, 103.77434207260274, 129.6713420948738, 111.53989919719618, 125.35096221957988, 133.80412534519596, 125.56184456486774, 112.27613909327106, 145.45316557540502, 132.84602855040126, 122.77309374981878, 123.51489216128934, 141.07156630615867, 86.32667997133046, 111.31850894957364, 125.20597952633936, 138.50479752160004, 136.0215086624037, 142.9267604900342, 114.73920858149144, 128.79633943577815, 133.35154976500274, 126.04485970807585, 147.9451673438497, 141.05390877929082, 134.1729503296255, 141.8811741844559, 132.38325132881317, 138.3502398132462, 127.69281397614108, 128.02964451642623, 139.13596229772332]\n",
      "Mean of Mean Squared Errors: 125.14525114866767\n",
      "Standard Deviation of Mean Squared Errors: 16.623092183854727\n"
     ]
    }
   ],
   "source": [
    "# increase the number of hiddern layers to 3 each of 10 nodes and ReLU activation function.\n",
    "mse_list = []\n",
    "\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(8,)))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "# Repeat the split and train steps 50 times\n",
    "for i in range(50):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = regression_model()\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Now this will work\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Print the list of mean squared errors\n",
    "print(mse_list)\n",
    "\n",
    "#  Report the mean and the standard deviation of the mean squared errors.\n",
    "\n",
    "mean_mse = np.mean(mse_list)  # Calculate the mean of the MSEs\n",
    "std_mse = np.std(mse_list)    # Calculate the standard deviation of the MSEs\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean of Mean Squared Errors: {mean_mse}\")\n",
    "print(f\"Standard Deviation of Mean Squared Errors: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the mean has been signgactly reduced to 125.145 after adding the new hidden layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
